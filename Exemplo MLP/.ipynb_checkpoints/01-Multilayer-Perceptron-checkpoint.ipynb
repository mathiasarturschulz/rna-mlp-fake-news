{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historicamente, Perceptron foi o nome dado a um modelo de rede neural com uma única camada linear. Se o modelo tem múltiplas camadas, chamamos de Perceptron Multicamada (MLP - Multilayer Perceprtron). Cada nó na primeira camada recebe uma entrada e dispara de acordo com os limites de decisão locais predefinidos (thresholds). Em seguida, a saída da primeira camada é passada para a segunda camada, cujos resultados são passados para a camada de saída final consistindo de um único neurônio. \n",
    "\n",
    "A rede pode ser densa, o que significa que cada neurônio em uma camada está conectado a todos os neurônios localizados na camada anterior e a todos os neurônios na camada seguinte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando Redes Neurais com Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar um único neurônio. Quais são as melhores escolhas para o peso w e o bias b? Idealmente, gostaríamos de fornecer um conjunto de exemplos de treinamento e deixar o computador ajustar o peso e o bias de tal forma que os erros produzidos na saída sejam minimizados. A fim de tornar isso um pouco mais concreto, vamos supor que temos um conjunto de imagens de gatos e outro conjunto separado de imagens que não contenham gatos. Por uma questão de simplicidade, suponha que cada neurônio olhe para um único valor de pixel de entrada. Enquanto o computador processa essas imagens, gostaríamos que nosso neurônio ajustasse seus pesos e bias para que tenhamos menos e menos imagens erroneamente reconhecidas como não-gatos. Essa abordagem parece muito intuitiva, mas exige que uma pequena alteração nos pesos (e/ou bias) cause apenas uma pequena mudança nas saídas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tivermos um grande salto de saída, não podemos aprender progressivamente. Afinal, as crianças aprendem pouco a pouco. Infelizmente, o perceptron não mostra esse comportamento \"pouco a pouco\". Um perceptron é 0 ou 1 e isso é um grande salto e não vai ajudá-lo a aprender. Precisamos de algo diferente, mais suave. Precisamos de uma função que mude progressivamente de 0 para 1 sem descontinuidade. Matematicamente, isso significa que precisamos de uma função contínua que nos permita calcular a derivada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada neurônio pode ser inicializado com pesos específicos. Keras oferece algumas opções, a mais comum são:\n",
    "\n",
    "Random_uniform: Os pesos são inicializados com valores uniformemente pequenos e aleatórios em (-0,05, 0,05). \n",
    "\n",
    "Random_normal: Os pesos são inicializados de acordo com uma distribuição Gaussiana, com média zero e pequeno desvio padrão de 0,05. \n",
    "\n",
    "Zero: Todos os pesos são inicializados para zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/uciml/pima-indians-diabetes-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset\n",
    "dataset = numpy.loadtxt(\"data.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprime o dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split em variáveis de input (X) e output (Y) \n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 67% for train and 33% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\danielgsoares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Cria o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural network 1 hidden layer](01-arquitetura-rede-neural.png \"Rede Neural com 1 Camada Oculta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Sigmóide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função sigmóide é uma função matemática de amplo uso em campos como a economia e a computação. O nome \"sigmóide\" vem da forma em S do seu gráfico. Um neurônio pode usar o sigmóide para calcular a função não-linear. Um neurônio com ativação sigmóide tem um comportamento semelhante ao perceptron, mas as mudanças são graduais e os valores de saída, como 0.3537 ou 0.147191, são perfeitamente legítimos. A função de ativação sigmóide é comumente utilizada por redes neurais com propagação positiva (Feedforward) que precisam ter como saída apenas números positivos, em redes neurais multicamadas e em outras redes com sinais contínuos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O sigmóide não é o único tipo de função de ativação suave usada para redes neurais. Recentemente, uma função muito simples chamada unidade linear rectificada (ReLU) tornou-se muito popular porque gera resultados experimentais muito bons. Uma ReLU é simplesmente definida como uma função não-linear e a função é zero para valores negativos e cresce linearmente para valores positivos.\n",
    "Sigmoid e ReLU são geralmente chamados funções de ativação das redes neurais. Essas mudanças graduais, típicas das funções Sigmóide e ReLU, são os blocos básicos para o desenvolvimento de um algoritmo de aprendizado que se adapta pouco a pouco, reduzindo progressivamente os erros cometidos pelas redes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilação do modelo\n",
    "# Precisamos selecionar o otimizador que é o algoritmo específico usado para atualizar pesos enquanto \n",
    "# treinamos nosso modelo.\n",
    "# Precisamos selecionar também a função objetivo que é usada pelo otimizador para navegar no espaço de pesos \n",
    "# (frequentemente, as funções objetivo são chamadas de função de perda (loss) e o processo de otimização é definido \n",
    "# como um processo de minimização de perdas).\n",
    "# Outras funções aqui: https://keras.io/losses/\n",
    "# A função objetivo \"categorical_crossentropy\" é a função objetivo adequada para predições de rótulos multiclass e \n",
    "# binary_crossentropy para classificação binária. \n",
    "# A métrica é usada para medir a performance do modelo. Outras métricas: https://keras.io/metrics/\n",
    "# As métricas são semelhantes às funções objetivo, com a única diferença de que elas não são usadas para \n",
    "# treinar um modelo, mas apenas para avaliar um modelo. \n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\danielgsoares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 1s 2ms/step - loss: 0.6749 - acc: 0.6401\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s 179us/step - loss: 0.6673 - acc: 0.6381\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.6561 - acc: 0.6381\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.6412 - acc: 0.6615\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.6353 - acc: 0.6206\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.6206 - acc: 0.6751\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.6169 - acc: 0.6732\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.6067 - acc: 0.6770\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.6017 - acc: 0.6984\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s 187us/step - loss: 0.6008 - acc: 0.6809\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.5919 - acc: 0.6868\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.5962 - acc: 0.6732\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s 257us/step - loss: 0.5946 - acc: 0.6732\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s 261us/step - loss: 0.5883 - acc: 0.6907\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s 216us/step - loss: 0.5911 - acc: 0.7062\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s 202us/step - loss: 0.5844 - acc: 0.6984\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s 241us/step - loss: 0.5837 - acc: 0.6946\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s 227us/step - loss: 0.5855 - acc: 0.6946\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.5863 - acc: 0.6732\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.5810 - acc: 0.7023\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.5809 - acc: 0.6946\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s 187us/step - loss: 0.5790 - acc: 0.6965\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.5829 - acc: 0.6829\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.5748 - acc: 0.6790\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s 161us/step - loss: 0.5919 - acc: 0.6829\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s 200us/step - loss: 0.5743 - acc: 0.6751\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.5708 - acc: 0.7062\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.5738 - acc: 0.6887\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.5690 - acc: 0.7101\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s 179us/step - loss: 0.5736 - acc: 0.7004\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.5662 - acc: 0.6984\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.5790 - acc: 0.7062\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.5695 - acc: 0.6965\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.5700 - acc: 0.7062\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s 229us/step - loss: 0.5728 - acc: 0.6926\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s 226us/step - loss: 0.5668 - acc: 0.7023\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s 181us/step - loss: 0.5643 - acc: 0.7043\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s 189us/step - loss: 0.5606 - acc: 0.7023\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s 237us/step - loss: 0.5667 - acc: 0.7082\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s 231us/step - loss: 0.5640 - acc: 0.7043\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s 214us/step - loss: 0.5559 - acc: 0.7198\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s 181us/step - loss: 0.5649 - acc: 0.6887\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.5527 - acc: 0.7062\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s 206us/step - loss: 0.5540 - acc: 0.7140\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s 206us/step - loss: 0.5625 - acc: 0.6946\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s 222us/step - loss: 0.5570 - acc: 0.7179\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.5514 - acc: 0.7121\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s 181us/step - loss: 0.5581 - acc: 0.7082\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5480 - acc: 0.7121\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s 189us/step - loss: 0.5529 - acc: 0.7140\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s 195us/step - loss: 0.5476 - acc: 0.7101\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s 198us/step - loss: 0.5490 - acc: 0.7140\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s 196us/step - loss: 0.5593 - acc: 0.7043\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.5466 - acc: 0.7198\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.5421 - acc: 0.7218\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.5481 - acc: 0.7140\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.5449 - acc: 0.7140\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s 198us/step - loss: 0.5370 - acc: 0.7101\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.5410 - acc: 0.7276\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.5356 - acc: 0.7257\n",
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s 218us/step - loss: 0.5409 - acc: 0.7354\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s 192us/step - loss: 0.5420 - acc: 0.7121\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s 267us/step - loss: 0.5358 - acc: 0.7335\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s 231us/step - loss: 0.5373 - acc: 0.7082\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s 240us/step - loss: 0.5540 - acc: 0.7121\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s 222us/step - loss: 0.5340 - acc: 0.7335\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.5288 - acc: 0.7257\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s 179us/step - loss: 0.5366 - acc: 0.7257\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.5362 - acc: 0.7101\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s 181us/step - loss: 0.5287 - acc: 0.7393\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.5238 - acc: 0.7374\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s 187us/step - loss: 0.5295 - acc: 0.7296\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s 198us/step - loss: 0.5244 - acc: 0.7276\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s 187us/step - loss: 0.5297 - acc: 0.7393\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.5204 - acc: 0.7393\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s 189us/step - loss: 0.5202 - acc: 0.7510\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s 196us/step - loss: 0.5177 - acc: 0.7257\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.5132 - acc: 0.7432\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.5125 - acc: 0.7393\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.5201 - acc: 0.7374\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.5360 - acc: 0.7315\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.5168 - acc: 0.7296\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s 179us/step - loss: 0.5144 - acc: 0.7412\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.5027 - acc: 0.7529\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s 150us/step - loss: 0.5132 - acc: 0.7432\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s 167us/step - loss: 0.5022 - acc: 0.7510\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.5119 - acc: 0.7432\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s 220us/step - loss: 0.5102 - acc: 0.7510 0s - loss: 0.4733 - acc: 0.77\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s 255us/step - loss: 0.5151 - acc: 0.7412\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s 368us/step - loss: 0.5066 - acc: 0.7529\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s 284us/step - loss: 0.5047 - acc: 0.7471\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s 204us/step - loss: 0.4988 - acc: 0.7626\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s 208us/step - loss: 0.4998 - acc: 0.7490\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s 245us/step - loss: 0.4936 - acc: 0.7626\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.4935 - acc: 0.7549\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4912 - acc: 0.7685\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.5103 - acc: 0.7490\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s 218us/step - loss: 0.5184 - acc: 0.7393\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s 229us/step - loss: 0.4958 - acc: 0.7549\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s 282us/step - loss: 0.4934 - acc: 0.7646\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.5100 - acc: 0.7626\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.5003 - acc: 0.7549\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s 179us/step - loss: 0.5006 - acc: 0.7685\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.4879 - acc: 0.7549\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4887 - acc: 0.7626\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s 227us/step - loss: 0.4926 - acc: 0.7588\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s 239us/step - loss: 0.4966 - acc: 0.7568\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s 264us/step - loss: 0.4841 - acc: 0.7704\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s 181us/step - loss: 0.4845 - acc: 0.7685\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.4718 - acc: 0.783 - 0s 185us/step - loss: 0.4992 - acc: 0.7510\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s 235us/step - loss: 0.4896 - acc: 0.7646\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.4969 - acc: 0.7549\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4892 - acc: 0.7724\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4986 - acc: 0.7665\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.4866 - acc: 0.7510\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s 237us/step - loss: 0.4766 - acc: 0.7724\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s 344us/step - loss: 0.4861 - acc: 0.7685\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s 175us/step - loss: 0.4825 - acc: 0.7704\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4839 - acc: 0.7665\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4808 - acc: 0.7588\n",
      "Epoch 121/150\n",
      "514/514 [==============================] - 0s 157us/step - loss: 0.4772 - acc: 0.7704\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s 154us/step - loss: 0.4868 - acc: 0.7782\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s 192us/step - loss: 0.4976 - acc: 0.7451\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s 189us/step - loss: 0.4758 - acc: 0.7879\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s 136us/step - loss: 0.4718 - acc: 0.7918\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s 159us/step - loss: 0.4828 - acc: 0.7665\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s 165us/step - loss: 0.4688 - acc: 0.7685\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.4761 - acc: 0.7840\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.4715 - acc: 0.7860\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s 270us/step - loss: 0.4826 - acc: 0.7685\n",
      "Epoch 131/150\n",
      "514/514 [==============================] - 0s 249us/step - loss: 0.4826 - acc: 0.7588\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.4702 - acc: 0.7879\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s 169us/step - loss: 0.4681 - acc: 0.7840\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s 192us/step - loss: 0.4703 - acc: 0.7879\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4684 - acc: 0.7763\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s 253us/step - loss: 0.4688 - acc: 0.7802\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4614 - acc: 0.7821\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.4939 - acc: 0.7763\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s 243us/step - loss: 0.4610 - acc: 0.7918\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s 206us/step - loss: 0.4710 - acc: 0.7704\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s 171us/step - loss: 0.4601 - acc: 0.7821\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.4675 - acc: 0.7860\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s 173us/step - loss: 0.4616 - acc: 0.7879\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s 163us/step - loss: 0.4676 - acc: 0.7704\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s 229us/step - loss: 0.4734 - acc: 0.7704\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s 231us/step - loss: 0.4648 - acc: 0.7763\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s 305us/step - loss: 0.4618 - acc: 0.7685\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s 208us/step - loss: 0.4563 - acc: 0.7879\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.4574 - acc: 0.7938\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.4539 - acc: 0.7938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27984869748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinamento do modelo\n",
    "# Epochs: Este é o número de vezes que o modelo é exposto ao conjunto de treinamento. Em cada iteração, \n",
    "# o otimizador tenta ajustar os pesos para que a função objetivo seja minimizada. \n",
    "# Batch_size: Esse é o número de instâncias de treinamento observadas antes que o otimizador execute uma \n",
    "# atualização de peso.\n",
    "model.fit(X_train, y_train, epochs = 150, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 0s 713us/step\n",
      "\n",
      "Loss: 0.51, Acurácia: 74.80%\n"
     ]
    }
   ],
   "source": [
    "# Avalia o modelo com os dados de teste\n",
    "# Uma vez treinado o modelo, podemos avaliá-lo no conjunto de testes que contém novos exemplos não vistos. \n",
    "# Desta forma, podemos obter o valor mínimo alcançado pela função objetivo e o melhor valor alcançado pela métrica \n",
    "# de avaliação. Note-se que o conjunto de treinamento e o conjunto de teste são rigorosamente separados. \n",
    "# Não vale a pena avaliar um modelo em um exemplo que já foi usado para treinamento. \n",
    "# A aprendizagem é essencialmente um processo destinado a generalizar observações invisíveis e não a memorizar \n",
    "# o que já é conhecido.\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Acurácia: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera as previsões\n",
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Acurácia das Previsões: 77.86%\n"
     ]
    }
   ],
   "source": [
    "# Ajusta as previsões e imprime o resultado\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)\n",
    "accuracy = numpy.mean(rounded == Y)\n",
    "print(\"Acurácia das Previsões: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
